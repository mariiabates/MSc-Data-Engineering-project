{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp38-cp38-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.8/site-packages (from matplotlib) (1.23.1)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.37.1-py3-none-any.whl (957 kB)\n",
      "\u001b[K     |████████████████████████████████| 957 kB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Using cached Pillow-9.2.0-cp38-cp38-macosx_10_10_x86_64.whl (3.1 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in ./env/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.37.1 kiwisolver-1.4.4 matplotlib-3.5.3 pillow-9.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/mse43/project/env/bin/python3.8-intel64 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "count  2225.000000\n",
      "mean    384.040449\n",
      "std     238.174497\n",
      "min      89.000000\n",
      "25%     246.000000\n",
      "50%     332.000000\n",
      "75%     471.000000\n",
      "max    4432.000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_num_of_words_in_text(path):\n",
    "    '''\n",
    "    Path should have folders indicating labels \n",
    "    with sorted .txt files containing text data.\n",
    "    \n",
    "    '''\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(subdir)\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if not file_path.endswith('txt'):\n",
    "                continue\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                words_in_text_num = len(text.split())\n",
    "                yield words_in_text_num\n",
    "\n",
    "df = pd.DataFrame(get_num_of_words_in_text(\"data/\"))\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = [None]\n",
    "# print(n[0])\n",
    "\n",
    "text = [\"Betty Who talks about her cool suit, what she feels like is the most iconic VMA moment, how much she\\u2019s loving Beyonc\\u00e9\\u2019s \\u2018Renaissance\\u2019 and more on the 2022 VMA red carpet.\"]\n",
    "len(text[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What now for British tennis?\\n\\nTim Henman\\'s decision to quit Davis Cup tennis has left the British team with a gargantuan void to fill.\\n\\nThe world number seven is tied for fourth among his countrymen for wins in the history of the tournament (he has 36 from his 50 rubbers). And Great Britain\\'s last Davis Cup win without Henman came against Slovenia as far back as 1996. Worse could follow, according to former British team member Chris Bailey. Bailey told BBC Sport: \"After Tim\\'s announcement, I doubt Greg Rusedski will be that far behind him.\" But without their top two, where does that leave British ambitions in the sport\\'s premier team event? Captain Jeremy Bates has singled out Alex Bogdanovic and Andrew Murray as potential replacements. The Yugoslavian-born Bogdanovic, though, is 184 places below Henman in the world rankings and has played just two cup ties - winning one and losing the other.\\n\\nMurray, on the other hand, is 407th in the current ATP entry list and yet to make his cup debut. But Bailey does see some hope for the future. He said: \"Now we\\'ve dropped down to the Euro-Africa zone, the time was right for him to step down and let the young guys come to the fore.\" Britain\\'s next opponents, Israel, are hardly likely to be quaking in their boots ahead of the 4-6 March match against a likely trio of Bogdanovic, Murray and the 187th-ranked Arvind Parmar. Bailey said: \"It will be tough for GB to move up, but there comes a time when our young players have to step up. This was always going to be inevitable with Tim and Greg\\'s growing years. \"I\\'m confident about the future. I wouldn\\'t lay money on us getting back into the world group next year, but I\\'d imagine in five years time we\\'ll be competing for the major honours.\" Of those lining up to replace Henman, the 17-year-old Murray, with four Futures titles under his belt last year, looks the best long-term bet. \"Murray is the one that looks likeliest to take over Tim\\'s mantle,\" said Bailey. \"He has an enormous amount of self-confidence, judging by what he\\'s said in the past.\" Bogdanovic, three years Murray\\'s senior, has had a more troubled time under Britain\\'s Davis Cup umbrella.\\n\\nWhile Murray has been marked out as Britain\\'s golden boy, Bogdanovic was warned by the Lawn Tennis Association for a lack of drive at the end of 2003. And Bailey said: \"Despite that, Alex is clearly talented as well, while Arvind is another contender. \"They\\'re among the guys who have experienced the intensity of Davis Cup tennis - whether as players or on the sidelines. \"The LTA has always done an exceptional job of ensuring that. \"Now they\\'ll finally get to play regularly in the cauldron of the cup. And I\\'m confident that will springboard Team GB to greater success.\"\\n']\n",
      "{'id': [998], 'label_1': ['sport']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>998</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id label_1\n",
       "0  998   sport"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "\n",
    "# data = {'id': [998], 'text': ['What now for British tennis?\\n\\nTim Henman\\'s decision to quit Davis Cup tennis has left the British team with a gargantuan void to fill.\\n\\nThe world number seven is tied for fourth among his countrymen for wins in the history of the tournament (he has 36 from his 50 rubbers). And Great Britain\\'s last Davis Cup win without Henman came against Slovenia as far back as 1996. Worse could follow, according to former British team member Chris Bailey. Bailey told BBC Sport: \"After Tim\\'s announcement, I doubt Greg Rusedski will be that far behind him.\" But without their top two, where does that leave British ambitions in the sport\\'s premier team event? Captain Jeremy Bates has singled out Alex Bogdanovic and Andrew Murray as potential replacements. The Yugoslavian-born Bogdanovic, though, is 184 places below Henman in the world rankings and has played just two cup ties - winning one and losing the other.\\n\\nMurray, on the other hand, is 407th in the current ATP entry list and yet to make his cup debut. But Bailey does see some hope for the future. He said: \"Now we\\'ve dropped down to the Euro-Africa zone, the time was right for him to step down and let the young guys come to the fore.\" Britain\\'s next opponents, Israel, are hardly likely to be quaking in their boots ahead of the 4-6 March match against a likely trio of Bogdanovic, Murray and the 187th-ranked Arvind Parmar. Bailey said: \"It will be tough for GB to move up, but there comes a time when our young players have to step up. This was always going to be inevitable with Tim and Greg\\'s growing years. \"I\\'m confident about the future. I wouldn\\'t lay money on us getting back into the world group next year, but I\\'d imagine in five years time we\\'ll be competing for the major honours.\" Of those lining up to replace Henman, the 17-year-old Murray, with four Futures titles under his belt last year, looks the best long-term bet. \"Murray is the one that looks likeliest to take over Tim\\'s mantle,\" said Bailey. \"He has an enormous amount of self-confidence, judging by what he\\'s said in the past.\" Bogdanovic, three years Murray\\'s senior, has had a more troubled time under Britain\\'s Davis Cup umbrella.\\n\\nWhile Murray has been marked out as Britain\\'s golden boy, Bogdanovic was warned by the Lawn Tennis Association for a lack of drive at the end of 2003. And Bailey said: \"Despite that, Alex is clearly talented as well, while Arvind is another contender. \"They\\'re among the guys who have experienced the intensity of Davis Cup tennis - whether as players or on the sidelines. \"The LTA has always done an exceptional job of ensuring that. \"Now they\\'ll finally get to play regularly in the cauldron of the cup. And I\\'m confident that will springboard Team GB to greater success.\"\\n'], 'label_1': ['sport']}\n",
    "# TypeError: Expected file path name or file-like object, got <class 'bytes'> type\n",
    "body = b'{\"id\": [998], \"text\": [\"What now for British tennis?\\\\n\\\\nTim Henman\\'s decision to quit Davis Cup tennis has left the British team with a gargantuan void to fill.\\\\n\\\\nThe world number seven is tied for fourth among his countrymen for wins in the history of the tournament (he has 36 from his 50 rubbers). And Great Britain\\'s last Davis Cup win without Henman came against Slovenia as far back as 1996. Worse could follow, according to former British team member Chris Bailey. Bailey told BBC Sport: \\\\\"After Tim\\'s announcement, I doubt Greg Rusedski will be that far behind him.\\\\\" But without their top two, where does that leave British ambitions in the sport\\'s premier team event? Captain Jeremy Bates has singled out Alex Bogdanovic and Andrew Murray as potential replacements. The Yugoslavian-born Bogdanovic, though, is 184 places below Henman in the world rankings and has played just two cup ties - winning one and losing the other.\\\\n\\\\nMurray, on the other hand, is 407th in the current ATP entry list and yet to make his cup debut. But Bailey does see some hope for the future. He said: \\\\\"Now we\\'ve dropped down to the Euro-Africa zone, the time was right for him to step down and let the young guys come to the fore.\\\\\" Britain\\'s next opponents, Israel, are hardly likely to be quaking in their boots ahead of the 4-6 March match against a likely trio of Bogdanovic, Murray and the 187th-ranked Arvind Parmar. Bailey said: \\\\\"It will be tough for GB to move up, but there comes a time when our young players have to step up. This was always going to be inevitable with Tim and Greg\\'s growing years. \\\\\"I\\'m confident about the future. I wouldn\\'t lay money on us getting back into the world group next year, but I\\'d imagine in five years time we\\'ll be competing for the major honours.\\\\\" Of those lining up to replace Henman, the 17-year-old Murray, with four Futures titles under his belt last year, looks the best long-term bet. \\\\\"Murray is the one that looks likeliest to take over Tim\\'s mantle,\\\\\" said Bailey. \\\\\"He has an enormous amount of self-confidence, judging by what he\\'s said in the past.\\\\\" Bogdanovic, three years Murray\\'s senior, has had a more troubled time under Britain\\'s Davis Cup umbrella.\\\\n\\\\nWhile Murray has been marked out as Britain\\'s golden boy, Bogdanovic was warned by the Lawn Tennis Association for a lack of drive at the end of 2003. And Bailey said: \\\\\"Despite that, Alex is clearly talented as well, while Arvind is another contender. \\\\\"They\\'re among the guys who have experienced the intensity of Davis Cup tennis - whether as players or on the sidelines. \\\\\"The LTA has always done an exceptional job of ensuring that. \\\\\"Now they\\'ll finally get to play regularly in the cauldron of the cup. And I\\'m confident that will springboard Team GB to greater success.\\\\\"\\\\n\"], \"label_1\": [\"sport\"]}'\n",
    "my_dict = json.loads(body) \n",
    "text = my_dict.pop('text', None)\n",
    "print(text)\n",
    "print(my_dict)\n",
    "\n",
    "ls = pd.DataFrame.from_dict(my_dict)\n",
    "# df = pd.read_json(body, dtype={\"text\": \"string\",})\n",
    "# df = df.set_index('id')\n",
    "# df\n",
    "# df['text_clean'] = clean_text(df['text'])\n",
    "\n",
    "# df = pd.DataFrame.from_dict(data)\n",
    "# df = df.set_index('id')\n",
    "# df = df.T\n",
    "# # df.columns = df.iloc[0]\n",
    "# df\n",
    "# trained_sentiment = TextBlob(df[\"text\"]).sentiment\n",
    "# df[\"text\"].iloc[0]\n",
    "# df.index[0]\n",
    "# # df.drop\n",
    "# # data.items()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following module \n",
    "# reads data stored in data\n",
    "# parses it\n",
    "# in a uniform way loads to the db\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Sequence, Text\n",
    "from sqlalchemy.dialects import postgresql\n",
    "from utilities import setup_logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "TABLE_NAME = 'contentful'\n",
    "CONNECTION_STRING = 'postgresql://postgres:postgres@localhost:5433/postgres'\n",
    "\n",
    "\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "def get_label_and_text(path):\n",
    "    '''\n",
    "    Path should have folders indicating labels \n",
    "    with sorted .txt files containing text data.\n",
    "    \n",
    "    '''\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(subdir)\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if not file_path.endswith('txt'):\n",
    "                continue\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                yield (parent_dir, text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = setup_logger()\n",
    "\n",
    "    labeled_data_generator = get_label_and_text(DATA_DIR)\n",
    "    raw_data_labeled = pd.DataFrame(labeled_data_generator, columns=[\"label_1\", \"text\"])\n",
    "    logger.info(f\"Read {len(raw_data_labeled)} rows of data\")  # 2225\n",
    "\n",
    "    X = raw_data_labeled[\"text\"]\n",
    "    y = raw_data_labeled[\"label_1\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=0.8, stratify=y)\n",
    "\n",
    "#     rows_inserted = raw_data_labeled.to_sql(\n",
    "#         name=f'{TABLE_NAME}', \n",
    "#         con=engine, \n",
    "#         if_exists='replace',\n",
    "#         index=False,\n",
    "#     )\n",
    "#     logger.info(f\"Inserted {len(rows_inserted)} rows of data\")\n",
    "\n",
    "#     with engine.connect() as con:\n",
    "#         con.execute(f'ALTER TABLE {TABLE_NAME} ADD COLUMN id SERIAL PRIMARY KEY;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following module \n",
    "# reads data stored in data\n",
    "# parses it\n",
    "# in a uniform way loads to the db\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Sequence, Text\n",
    "from sqlalchemy.dialects import postgresql\n",
    "from utilities import setup_logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "TABLE_NAME = 'contentful'\n",
    "CONNECTION_STRING = 'postgresql://postgres:postgres@localhost:5433/postgres'\n",
    "\n",
    "\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "def get_label_and_text(path):\n",
    "    '''\n",
    "    Path should have folders indicating labels \n",
    "    with sorted .txt files containing text data.\n",
    "    \n",
    "    '''\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(subdir)\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if not file_path.endswith('txt'):\n",
    "                continue\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                yield (parent_dir, text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = setup_logger()\n",
    "\n",
    "    labeled_data_generator = get_label_and_text(DATA_DIR)\n",
    "    raw_data_labeled = pd.DataFrame(labeled_data_generator, columns=[\"label_1\", \"text\"])\n",
    "    logger.info(f\"Read {len(raw_data_labeled)} rows of data\")  # 2225\n",
    "\n",
    "    X = raw_data_labeled[\"text\"]\n",
    "    y = raw_data_labeled[\"label_1\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=0.8, stratify=y)\n",
    "X_train\n",
    "#     rows_inserted = raw_data_labeled.to_sql(\n",
    "#         name=f'{TABLE_NAME}', \n",
    "#         con=engine, \n",
    "#         if_exists='replace',\n",
    "#         index=False,\n",
    "#     )\n",
    "#     logger.info(f\"Inserted {len(rows_inserted)} rows of data\")\n",
    "\n",
    "#     with engine.connect() as con:\n",
    "#         con.execute(f'ALTER TABLE {TABLE_NAME} ADD COLUMN id SERIAL PRIMARY KEY;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA CLASS DISTRIBUTION:\n",
      "label_1\n",
      "business         408\n",
      "entertainment    309\n",
      "politics         333\n",
      "sport            409\n",
      "tech             321\n",
      "dtype: int64\n",
      "TOTAL: 1780\n",
      "\n",
      "TEST_DATA CLASS DISTRIBUTION:\n",
      "label_1\n",
      "business         102\n",
      "entertainment     77\n",
      "politics          84\n",
      "sport            102\n",
      "tech              80\n",
      "dtype: int64\n",
      "TOTAL: 445\n"
     ]
    }
   ],
   "source": [
    "### The following module \n",
    "# reads data stored in data\n",
    "# parses it\n",
    "# in a uniform way loads to the db\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Sequence, Text\n",
    "from sqlalchemy.dialects import postgresql\n",
    "# from utilities import setup_logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "TABLE_NAME = 'contentful'\n",
    "CONNECTION_STRING = 'postgresql://postgres:postgres@localhost:5433/postgres'\n",
    "\n",
    "\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "def get_label_and_text(path):\n",
    "    '''\n",
    "    Path should have folders indicating labels \n",
    "    with sorted .txt files containing text data.\n",
    "    \n",
    "    '''\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(subdir)\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if not file_path.endswith('txt'):\n",
    "                continue\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                yield (parent_dir, text)\n",
    "\n",
    "\n",
    "labeled_data_generator = get_label_and_text(DATA_DIR)\n",
    "raw_data_labeled = pd.DataFrame(labeled_data_generator, columns=[\"label_1\", \"text\"])\n",
    "#     logger.info(f\"Read {len(raw_data_labeled)} rows of data\")  # 2225\n",
    "\n",
    "#     X = raw_data_labeled[\"text\"]\n",
    "#     y = raw_data_labeled[\"label_1\"]\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    raw_data_labeled, train_size=0.8, stratify=raw_data_labeled[\"label_1\"], random_state=777)\n",
    "\n",
    "\n",
    "#     train_data = pd.concat([X_train, y_train], axis=1)\n",
    "#     test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#     print(type(X_train))\n",
    "#     print(X_train.head())\n",
    "#     print(y_train.head())\n",
    "# print(train_data.head())\n",
    "\n",
    "#     print(type(y_train))\n",
    "\n",
    "#     rows_inserted = train_data.to_sql(\n",
    "#         name='training_data', \n",
    "#         con=engine, \n",
    "#         if_exists='append',\n",
    "#         index_label=\"id\"\n",
    "#     )\n",
    "#     logger.info(f\"Inserted {rows_inserted} rows of data\")\n",
    "\n",
    "#     rows_inserted = test_data.to_sql(\n",
    "#         name='testing_data', \n",
    "#         con=engine, \n",
    "#         if_exists='append',\n",
    "#         index_label=\"id\"\n",
    "#     )\n",
    "#     logger.info(f\"Inserted {rows_inserted} rows of data\")\n",
    "\n",
    "#     with engine.connect() as con:\n",
    "#         con.execute(f'ALTER TABLE {TABLE_NAME} ADD COLUMN id SERIAL PRIMARY KEY;')\n",
    "\n",
    "\n",
    "print(\"TRAIN_DATA CLASS DISTRIBUTION:\")\n",
    "print(f\"{train_data.groupby(['label_1']).size()}\")\n",
    "print(f\"TOTAL: {len(train_data)}\")\n",
    "\n",
    "print(\"\\nTEST_DATA CLASS DISTRIBUTION:\")\n",
    "print(f\"{test_data.groupby(['label_1']).size()}\")\n",
    "print(f\"TOTAL: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "!pip install Scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "  \n",
    "### Get raw_data_labeled DF\n",
    "\n",
    "data_dir = \"/Users/Maria/Downloads/bbc_3\"\n",
    "\n",
    "\n",
    "# def read_txt_file(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         yield parent_dir, f.read()\n",
    "  \n",
    "\n",
    "# for folder in os.listdir():\n",
    "#     print(folder)\n",
    "#     os.chdir(folder)\n",
    "#     for folder in os.listdir():\n",
    "#         print(folder)\n",
    "#         os.chdir(path)\n",
    "   \n",
    "def get_label_and_text(path):\n",
    "    '''\n",
    "    Path should have folders indicating labels \n",
    "    with sorted .txt files containing text data.\n",
    "    \n",
    "    '''\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            parent_dir = os.path.basename(subdir)\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            if not file_path.endswith('txt'):\n",
    "                continue\n",
    "            with open(file_path) as f:\n",
    "                text = f.read()\n",
    "                yield (parent_dir, text)\n",
    "    \n",
    "\n",
    "labeled_data_generator = get_label_and_text(data_dir)\n",
    "\n",
    "# for i in label_txt_generator:\n",
    "#     print(i)\n",
    "\n",
    "raw_data_labeled = pd.DataFrame(labeled_data_generator, columns=[\"label\", \"text_raw\"])\n",
    "\n",
    "print(len(raw_data_labeled))  # 2225\n",
    "\n",
    "raw_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data_labeled\n",
    "df['text_raw']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('stopwords')\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEAN AND TOKENIZE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "   \n",
    "import time\n",
    "\n",
    "\n",
    "def clean_text(column):\n",
    "    start = time.time()\n",
    "    \n",
    "    # convert to lower case\n",
    "    column = column.str.lower()\n",
    "    \n",
    "#     column = column.apply(lambda row: [i.lower() for i in row])\n",
    "#     column = column.apply(word_tokenize)\n",
    "    \n",
    "#     column = column.apply(translate(None, string.punctuation))\n",
    "\n",
    "#     column = column.str.replace('[^\\w\\s]','', regex=True)\n",
    "#     column = column.str.replace('\\n','', regex=True)\n",
    "    column = column.str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "    column = column.str.replace('\\n',' ', regex=True)\n",
    "#     column = column.str.replace('\\n','', regex=True)\n",
    "    # split into words\n",
    "\n",
    "\n",
    "    column = column.apply(word_tokenize)\n",
    "    \n",
    "\n",
    "    # remove punctuation from each word\n",
    "\n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in tokens]\n",
    "#     column = column.apply(lambda x: x.translate(string.punctuation))\n",
    "\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    column = column.apply(lambda words: [w for w in words if not w in stop_words])\n",
    "    \n",
    "    \n",
    "    column = column.apply(lambda words: [PorterStemmer().stem(word) for word in words]) \n",
    "#     column = column.astype(str)\n",
    "    \n",
    "    end = time.time()                      \n",
    "#     print(end - start)\n",
    "    return column.astype(str)\n",
    "                          \n",
    "   \n",
    "    \n",
    "# df['text_clean'] = clean_text(df['text_raw']) #.astype(str)\n",
    "# df['text_clean']\n",
    "\n",
    "# # df['text_clean'] = df['text_raw'].apply(lambda column: clean_text(column))\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df[\"label_nums\"] = LabelEncoder().fit_transform(df[\"label\"])\n",
    "df.tail()\n",
    "\n",
    "\n",
    "# df[\"label_nums\"].nunique()\n",
    "\n",
    "X = df[\"text_clean\"]\n",
    "y = df[\"label_nums\"]\n",
    "print(X)\n",
    "print(y)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing\n",
    "print(\"hi\")\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(test_size=0.2, random_state=777)\n",
    "\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "# X_train, y_train, X_test, y_test = iterative_train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# REF: http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html\n",
    "# k_fold = IterativeStratification(n_splits=4, order=1)\n",
    "# for train, test in k_fold.split(X, y):\n",
    "#     print(train, test)\n",
    "#     classifier.fit(X[train], y[train])\n",
    "#     result = classifier.predict(X[test])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, stratify=y)\n",
    "# print(len(X_test))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                               text        label_1  \\\n",
      "0   359  U2 stars enter rock Hall of Fame\\n\\nSinger Bru...  entertainment   \n",
      "1  1736  Blair pledges unity to Labour MPs\\n\\nTony Blai...       politics   \n",
      "2   464  Brazil approves bankruptcy reform\\n\\nA major r...       business   \n",
      "3  1350  Campese berates whingeing England\\n\\nFormer Au...          sport   \n",
      "4  1107  Saint-Andre anger at absent stars\\n\\nSale Shar...          sport   \n",
      "\n",
      "  label_2                                         text_clean  \n",
      "0    None  ['u2', 'star', 'enter', 'rock', 'hall', 'fame'...  \n",
      "1    None  ['blair', 'pledg', 'uniti', 'labour', 'mp', 't...  \n",
      "2    None  ['brazil', 'approv', 'bankruptci', 'reform', '...  \n",
      "3    None  ['campes', 'berat', 'whing', 'england', 'forme...  \n",
      "4    None  ['saintandr', 'anger', 'absent', 'star', 'sale...  \n"
     ]
    }
   ],
   "source": [
    "## READ FROM TRAIN DB AND TEST DB + CLEAN\n",
    "import pandas as pd \n",
    "from sqlalchemy import create_engine\n",
    "from utilities import Postgres\n",
    "CONNECTION_STRING = 'postgresql://postgres:postgres@localhost:5433/postgres'\n",
    "\n",
    "cnx = Postgres(CONNECTION_STRING).get_cursor()\n",
    "\n",
    "# cnx = create_engine(CONNECTION_STRING).connect()\n",
    "  \n",
    "# table named 'contacts' will be returned as a dataframe.\n",
    "df_train = pd.read_sql_table('training_data', cnx)\n",
    "df_test = pd.read_sql_table('testing_data', cnx)\n",
    "# print(df.head())\n",
    "\n",
    "df_train['text_clean'] = clean_text(df_train['text'])\n",
    "df_test['text_clean'] = clean_text(df_test['text'])\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[\"text_clean\"].astype(str)\n",
    "y_train = df_train[\"label_1\"].astype(str)\n",
    "X_test = df_test[\"text_clean\"].astype(str)\n",
    "y_test = df_test[\"label_1\"].astype(str)\n",
    "\n",
    "X_train, y_train, X_test, y_test\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature extraction = Vectorization and Train\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(X_train, y_train)\n",
    "# # vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "\n",
    "classifier_pipe = Pipeline([('tfidf', TfidfVectorizer(preprocessor=None, tokenizer=None)),\n",
    "               ('regression', LogisticRegression()),\n",
    "              ])\n",
    "classifier_pipe.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "test_predict = classifier_pipe.predict(X_test)\n",
    "test_predict = le.inverse_transform(test_predict)\n",
    "\n",
    "# train_accuracy = round(classifier.score(X_train,y_train)*100)\n",
    "test_accuracy =round(accuracy_score(test_predict, y_test)*100)\n",
    "\n",
    "# test_accuracy\n",
    "test_predict = le.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entertainment', 'politics', 'business', 'sport', 'sport',\n",
       "       'business', 'politics', 'sport', 'sport', 'entertainment',\n",
       "       'business', 'sport', 'business', 'politics', 'politics', 'tech',\n",
       "       'politics', 'entertainment', 'sport', 'politics', 'entertainment',\n",
       "       'entertainment', 'sport', 'entertainment', 'sport', 'sport',\n",
       "       'entertainment', 'business', 'sport', 'sport', 'entertainment',\n",
       "       'politics', 'business', 'entertainment', 'business',\n",
       "       'entertainment', 'tech', 'business', 'business', 'business',\n",
       "       'business', 'sport', 'politics', 'sport', 'tech', 'business',\n",
       "       'entertainment', 'politics', 'business', 'entertainment',\n",
       "       'business', 'entertainment', 'entertainment', 'politics',\n",
       "       'entertainment', 'sport', 'sport', 'business', 'tech', 'business',\n",
       "       'tech', 'tech', 'sport', 'politics', 'sport', 'entertainment',\n",
       "       'entertainment', 'tech', 'sport', 'sport', 'entertainment', 'tech',\n",
       "       'entertainment', 'tech', 'sport', 'tech', 'tech', 'business',\n",
       "       'sport', 'entertainment', 'politics', 'sport', 'sport', 'business',\n",
       "       'sport', 'politics', 'business', 'tech', 'tech', 'politics',\n",
       "       'sport', 'sport', 'business', 'politics', 'tech', 'politics',\n",
       "       'tech', 'business', 'tech', 'sport', 'tech', 'sport', 'sport',\n",
       "       'entertainment', 'business', 'sport', 'sport', 'sport', 'politics',\n",
       "       'tech', 'business', 'business', 'politics', 'entertainment',\n",
       "       'politics', 'sport', 'tech', 'sport', 'business', 'entertainment',\n",
       "       'entertainment', 'tech', 'tech', 'business', 'business', 'sport',\n",
       "       'sport', 'sport', 'business', 'politics', 'business', 'sport',\n",
       "       'sport', 'entertainment', 'politics', 'business', 'tech',\n",
       "       'politics', 'tech', 'sport', 'business', 'business', 'sport',\n",
       "       'politics', 'entertainment', 'politics', 'politics',\n",
       "       'entertainment', 'sport', 'entertainment', 'business', 'sport',\n",
       "       'sport', 'entertainment', 'tech', 'tech', 'business', 'business',\n",
       "       'sport', 'politics', 'sport', 'politics', 'entertainment', 'sport',\n",
       "       'politics', 'politics', 'tech', 'entertainment', 'sport', 'sport',\n",
       "       'business', 'politics', 'business', 'entertainment',\n",
       "       'entertainment', 'sport', 'politics', 'tech', 'sport', 'politics',\n",
       "       'entertainment', 'entertainment', 'business', 'tech', 'tech',\n",
       "       'sport', 'entertainment', 'politics', 'entertainment', 'tech',\n",
       "       'business', 'tech', 'business', 'business', 'politics', 'business',\n",
       "       'business', 'tech', 'business', 'entertainment', 'tech',\n",
       "       'business', 'sport', 'tech', 'business', 'entertainment',\n",
       "       'entertainment', 'business', 'entertainment', 'business', 'sport',\n",
       "       'politics', 'business', 'entertainment', 'tech', 'tech',\n",
       "       'business', 'sport', 'business', 'sport', 'business', 'politics',\n",
       "       'entertainment', 'business', 'business', 'entertainment', 'sport',\n",
       "       'politics', 'tech', 'entertainment', 'sport', 'tech', 'tech',\n",
       "       'tech', 'business', 'sport', 'sport', 'sport', 'sport',\n",
       "       'entertainment', 'politics', 'sport', 'business', 'business',\n",
       "       'sport', 'politics', 'entertainment', 'entertainment', 'politics',\n",
       "       'politics', 'business', 'entertainment', 'tech', 'business',\n",
       "       'sport', 'entertainment', 'politics', 'entertainment',\n",
       "       'entertainment', 'politics', 'politics', 'business', 'tech',\n",
       "       'entertainment', 'sport', 'sport', 'sport', 'sport', 'politics',\n",
       "       'politics', 'entertainment', 'sport', 'business', 'business',\n",
       "       'tech', 'tech', 'sport', 'tech', 'entertainment', 'sport',\n",
       "       'politics', 'entertainment', 'business', 'politics', 'business',\n",
       "       'tech', 'sport', 'sport', 'business', 'business', 'tech', 'sport',\n",
       "       'tech', 'business', 'business', 'entertainment', 'business',\n",
       "       'politics', 'entertainment', 'entertainment', 'politics',\n",
       "       'politics', 'politics', 'entertainment', 'tech', 'business',\n",
       "       'business', 'tech', 'sport', 'politics', 'entertainment',\n",
       "       'business', 'business', 'politics', 'entertainment', 'politics',\n",
       "       'politics', 'sport', 'tech', 'entertainment', 'sport', 'tech',\n",
       "       'business', 'entertainment', 'tech', 'tech', 'sport', 'sport',\n",
       "       'sport', 'sport', 'business', 'entertainment', 'business',\n",
       "       'entertainment', 'politics', 'sport', 'tech', 'tech', 'tech',\n",
       "       'entertainment', 'politics', 'entertainment', 'business',\n",
       "       'politics', 'tech', 'tech', 'politics', 'entertainment',\n",
       "       'business', 'tech', 'entertainment', 'sport', 'tech',\n",
       "       'entertainment', 'sport', 'entertainment', 'politics', 'tech',\n",
       "       'politics', 'entertainment', 'sport', 'tech', 'politics',\n",
       "       'politics', 'sport', 'sport', 'politics', 'politics', 'business',\n",
       "       'politics', 'sport', 'tech', 'politics', 'business', 'business',\n",
       "       'politics', 'sport', 'business', 'tech', 'politics', 'tech',\n",
       "       'business', 'sport', 'business', 'tech', 'tech', 'sport', 'tech',\n",
       "       'politics', 'politics', 'entertainment', 'entertainment',\n",
       "       'business', 'politics', 'sport', 'business', 'tech', 'politics',\n",
       "       'sport', 'sport', 'business', 'business', 'politics', 'business',\n",
       "       'tech', 'tech', 'entertainment', 'entertainment', 'tech', 'sport',\n",
       "       'business', 'politics', 'business', 'tech', 'business', 'business',\n",
       "       'business', 'sport', 'business', 'sport', 'tech', 'business',\n",
       "       'business', 'business', 'politics', 'sport', 'tech', 'tech',\n",
       "       'politics', 'politics', 'sport', 'business', 'business', 'tech',\n",
       "       'sport', 'politics', 'business', 'politics', 'sport',\n",
       "       'entertainment', 'entertainment', 'entertainment', 'politics',\n",
       "       'business', 'sport'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "print(\"hi\")\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac0d49714e68a95b46301f6f42fa7856180e060e0d040b93c094694c7e90a95c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
